{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from utils import load_dataset\n",
    "from sklearn.preprocessing import OneHotEncoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, val_x, train_y, test_y, val_y, x_mean, x_std = load_dataset()\n",
    "N = train_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = tf.one_hot(train_y,depth=2)\n",
    "test_y_one_hot = tf.one_hot(test_y,depth=2)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate= 0.005)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('models/lr.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4598 - accuracy: 0.1125\n",
      "Epoch 00001: val_loss improved from inf to 0.31482, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 1.4598 - accuracy: 0.1125 - val_loss: 0.3148 - val_accuracy: 0.9100\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.9000\n",
      "Epoch 00002: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3534 - accuracy: 0.9000 - val_loss: 0.4617 - val_accuracy: 0.9100\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.9000\n",
      "Epoch 00003: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.5234 - accuracy: 0.9000 - val_loss: 0.5807 - val_accuracy: 0.9100\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6596 - accuracy: 0.9000\n",
      "Epoch 00004: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6596 - accuracy: 0.9000 - val_loss: 0.6455 - val_accuracy: 0.9100\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7359 - accuracy: 0.9000\n",
      "Epoch 00005: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7359 - accuracy: 0.9000 - val_loss: 0.6678 - val_accuracy: 0.9100\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7648 - accuracy: 0.9000\n",
      "Epoch 00006: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7648 - accuracy: 0.9000 - val_loss: 0.6567 - val_accuracy: 0.9100\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7563 - accuracy: 0.9000\n",
      "Epoch 00007: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.7563 - accuracy: 0.9000 - val_loss: 0.6183 - val_accuracy: 0.9100\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7172 - accuracy: 0.9000\n",
      "Epoch 00008: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.7172 - accuracy: 0.9000 - val_loss: 0.5577 - val_accuracy: 0.9100\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.9000\n",
      "Epoch 00009: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6528 - accuracy: 0.9000 - val_loss: 0.4797 - val_accuracy: 0.9100\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5684 - accuracy: 0.9000\n",
      "Epoch 00010: val_loss did not improve from 0.31482\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5684 - accuracy: 0.9000 - val_loss: 0.3922 - val_accuracy: 0.9100\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.9000\n",
      "Epoch 00011: val_loss improved from 0.31482 to 0.31098, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4710 - accuracy: 0.9000 - val_loss: 0.3110 - val_accuracy: 0.9300\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9075\n",
      "Epoch 00012: val_loss improved from 0.31098 to 0.26531, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3765 - accuracy: 0.9075 - val_loss: 0.2653 - val_accuracy: 0.9350\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.9150\n",
      "Epoch 00013: val_loss did not improve from 0.26531\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3186 - accuracy: 0.9150 - val_loss: 0.2848 - val_accuracy: 0.8950\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8775\n",
      "Epoch 00014: val_loss did not improve from 0.26531\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3340 - accuracy: 0.8775 - val_loss: 0.3463 - val_accuracy: 0.8150\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3989 - accuracy: 0.8175\n",
      "Epoch 00015: val_loss did not improve from 0.26531\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3989 - accuracy: 0.8175 - val_loss: 0.3751 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.7850\n",
      "Epoch 00016: val_loss did not improve from 0.26531\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4313 - accuracy: 0.7850 - val_loss: 0.3343 - val_accuracy: 0.8150\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3915 - accuracy: 0.8037\n",
      "Epoch 00017: val_loss improved from 0.26531 to 0.26018, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3915 - accuracy: 0.8037 - val_loss: 0.2602 - val_accuracy: 0.8950\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.8650\n",
      "Epoch 00018: val_loss improved from 0.26018 to 0.20758, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3183 - accuracy: 0.8650 - val_loss: 0.2076 - val_accuracy: 0.9450\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.9237\n",
      "Epoch 00019: val_loss improved from 0.20758 to 0.19249, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2693 - accuracy: 0.9237 - val_loss: 0.1925 - val_accuracy: 0.9450\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9237\n",
      "Epoch 00020: val_loss did not improve from 0.19249\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2607 - accuracy: 0.9237 - val_loss: 0.1989 - val_accuracy: 0.9400\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9162\n",
      "Epoch 00021: val_loss did not improve from 0.19249\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2745 - accuracy: 0.9162 - val_loss: 0.2088 - val_accuracy: 0.9300\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9112\n",
      "Epoch 00022: val_loss did not improve from 0.19249\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.2905 - accuracy: 0.9112 - val_loss: 0.2125 - val_accuracy: 0.9300\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.9100\n",
      "Epoch 00023: val_loss did not improve from 0.19249\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.2979 - accuracy: 0.9100 - val_loss: 0.2070 - val_accuracy: 0.9300\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.9100\n",
      "Epoch 00024: val_loss did not improve from 0.19249\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2934 - accuracy: 0.9100 - val_loss: 0.1939 - val_accuracy: 0.9350\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9100\n",
      "Epoch 00025: val_loss improved from 0.19249 to 0.17807, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2789 - accuracy: 0.9100 - val_loss: 0.1781 - val_accuracy: 0.9400\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9137\n",
      "Epoch 00026: val_loss improved from 0.17807 to 0.16729, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2599 - accuracy: 0.9137 - val_loss: 0.1673 - val_accuracy: 0.9450\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9125\n",
      "Epoch 00027: val_loss did not improve from 0.16729\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2450 - accuracy: 0.9125 - val_loss: 0.1682 - val_accuracy: 0.9450\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9200\n",
      "Epoch 00028: val_loss did not improve from 0.16729\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2417 - accuracy: 0.9200 - val_loss: 0.1802 - val_accuracy: 0.9550\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9162\n",
      "Epoch 00029: val_loss did not improve from 0.16729\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2498 - accuracy: 0.9162 - val_loss: 0.1930 - val_accuracy: 0.9550\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.9150\n",
      "Epoch 00030: val_loss did not improve from 0.16729\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2595 - accuracy: 0.9150 - val_loss: 0.1951 - val_accuracy: 0.9550\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9112\n",
      "Epoch 00031: val_loss did not improve from 0.16729\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2592 - accuracy: 0.9112 - val_loss: 0.1849 - val_accuracy: 0.9550\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.9187\n",
      "Epoch 00032: val_loss did not improve from 0.16729\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2476 - accuracy: 0.9187 - val_loss: 0.1707 - val_accuracy: 0.9450\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2329 - accuracy: 0.9212\n",
      "Epoch 00033: val_loss improved from 0.16729 to 0.16117, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2329 - accuracy: 0.9212 - val_loss: 0.1612 - val_accuracy: 0.9450\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9200\n",
      "Epoch 00034: val_loss improved from 0.16117 to 0.15883, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2235 - accuracy: 0.9200 - val_loss: 0.1588 - val_accuracy: 0.9400\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9175\n",
      "Epoch 00035: val_loss did not improve from 0.15883\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2214 - accuracy: 0.9175 - val_loss: 0.1607 - val_accuracy: 0.9450\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9162\n",
      "Epoch 00036: val_loss did not improve from 0.15883\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2227 - accuracy: 0.9162 - val_loss: 0.1627 - val_accuracy: 0.9450\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9162\n",
      "Epoch 00037: val_loss did not improve from 0.15883\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2228 - accuracy: 0.9162 - val_loss: 0.1621 - val_accuracy: 0.9450\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9175\n",
      "Epoch 00038: val_loss did not improve from 0.15883\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2192 - accuracy: 0.9175 - val_loss: 0.1592 - val_accuracy: 0.9450\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9175\n",
      "Epoch 00039: val_loss improved from 0.15883 to 0.15580, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2123 - accuracy: 0.9175 - val_loss: 0.1558 - val_accuracy: 0.9500\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9200\n",
      "Epoch 00040: val_loss improved from 0.15580 to 0.15463, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2045 - accuracy: 0.9200 - val_loss: 0.1546 - val_accuracy: 0.9450\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9200\n",
      "Epoch 00041: val_loss did not improve from 0.15463\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1994 - accuracy: 0.9200 - val_loss: 0.1569 - val_accuracy: 0.9450\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9237\n",
      "Epoch 00042: val_loss did not improve from 0.15463\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1986 - accuracy: 0.9237 - val_loss: 0.1608 - val_accuracy: 0.9300\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9237\n",
      "Epoch 00043: val_loss did not improve from 0.15463\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2003 - accuracy: 0.9237 - val_loss: 0.1631 - val_accuracy: 0.9350\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9300\n",
      "Epoch 00044: val_loss did not improve from 0.15463\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2009 - accuracy: 0.9300 - val_loss: 0.1619 - val_accuracy: 0.9400\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9350\n",
      "Epoch 00045: val_loss did not improve from 0.15463\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1981 - accuracy: 0.9350 - val_loss: 0.1586 - val_accuracy: 0.9500\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9300\n",
      "Epoch 00046: val_loss did not improve from 0.15463\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1934 - accuracy: 0.9300 - val_loss: 0.1557 - val_accuracy: 0.9450\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9275\n",
      "Epoch 00047: val_loss improved from 0.15463 to 0.15459, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1896 - accuracy: 0.9275 - val_loss: 0.1546 - val_accuracy: 0.9500\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9200\n",
      "Epoch 00048: val_loss did not improve from 0.15459\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1880 - accuracy: 0.9200 - val_loss: 0.1548 - val_accuracy: 0.9500\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9212\n",
      "Epoch 00049: val_loss did not improve from 0.15459\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1879 - accuracy: 0.9212 - val_loss: 0.1547 - val_accuracy: 0.9500\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9225\n",
      "Epoch 00050: val_loss improved from 0.15459 to 0.15343, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1874 - accuracy: 0.9225 - val_loss: 0.1534 - val_accuracy: 0.9500\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9237\n",
      "Epoch 00051: val_loss improved from 0.15343 to 0.15122, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.1854 - accuracy: 0.9237 - val_loss: 0.1512 - val_accuracy: 0.9500\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9225\n",
      "Epoch 00052: val_loss improved from 0.15122 to 0.14903, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1822 - accuracy: 0.9225 - val_loss: 0.1490 - val_accuracy: 0.9500\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9212\n",
      "Epoch 00053: val_loss improved from 0.14903 to 0.14787, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1788 - accuracy: 0.9212 - val_loss: 0.1479 - val_accuracy: 0.9450\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9275\n",
      "Epoch 00054: val_loss improved from 0.14787 to 0.14780, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.1766 - accuracy: 0.9275 - val_loss: 0.1478 - val_accuracy: 0.9500\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9325\n",
      "Epoch 00055: val_loss did not improve from 0.14780\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1755 - accuracy: 0.9325 - val_loss: 0.1479 - val_accuracy: 0.9500\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9400\n",
      "Epoch 00056: val_loss improved from 0.14780 to 0.14719, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1747 - accuracy: 0.9400 - val_loss: 0.1472 - val_accuracy: 0.9500\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9350\n",
      "Epoch 00057: val_loss improved from 0.14719 to 0.14545, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1733 - accuracy: 0.9350 - val_loss: 0.1454 - val_accuracy: 0.9450\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9300\n",
      "Epoch 00058: val_loss improved from 0.14545 to 0.14336, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1711 - accuracy: 0.9300 - val_loss: 0.1434 - val_accuracy: 0.9450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9275\n",
      "Epoch 00059: val_loss improved from 0.14336 to 0.14168, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1689 - accuracy: 0.9275 - val_loss: 0.1417 - val_accuracy: 0.9450\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9300\n",
      "Epoch 00060: val_loss improved from 0.14168 to 0.14058, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1674 - accuracy: 0.9300 - val_loss: 0.1406 - val_accuracy: 0.9450\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9275\n",
      "Epoch 00061: val_loss improved from 0.14058 to 0.13974, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1665 - accuracy: 0.9275 - val_loss: 0.1397 - val_accuracy: 0.9400\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9275\n",
      "Epoch 00062: val_loss improved from 0.13974 to 0.13886, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1657 - accuracy: 0.9275 - val_loss: 0.1389 - val_accuracy: 0.9400\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9275\n",
      "Epoch 00063: val_loss improved from 0.13886 to 0.13795, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1645 - accuracy: 0.9275 - val_loss: 0.1380 - val_accuracy: 0.9400\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9300\n",
      "Epoch 00064: val_loss improved from 0.13795 to 0.13728, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.1630 - accuracy: 0.9300 - val_loss: 0.1373 - val_accuracy: 0.9500\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9312\n",
      "Epoch 00065: val_loss improved from 0.13728 to 0.13703, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1614 - accuracy: 0.9312 - val_loss: 0.1370 - val_accuracy: 0.9550\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9312\n",
      "Epoch 00066: val_loss improved from 0.13703 to 0.13701, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1601 - accuracy: 0.9312 - val_loss: 0.1370 - val_accuracy: 0.9550\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9300\n",
      "Epoch 00067: val_loss improved from 0.13701 to 0.13683, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.1591 - accuracy: 0.9300 - val_loss: 0.1368 - val_accuracy: 0.9550\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9325\n",
      "Epoch 00068: val_loss improved from 0.13683 to 0.13619, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1581 - accuracy: 0.9325 - val_loss: 0.1362 - val_accuracy: 0.9550\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9337\n",
      "Epoch 00069: val_loss improved from 0.13619 to 0.13517, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1569 - accuracy: 0.9337 - val_loss: 0.1352 - val_accuracy: 0.9550\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9325\n",
      "Epoch 00070: val_loss improved from 0.13517 to 0.13405, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1555 - accuracy: 0.9325 - val_loss: 0.1340 - val_accuracy: 0.9550\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9325\n",
      "Epoch 00071: val_loss improved from 0.13405 to 0.13309, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1542 - accuracy: 0.9325 - val_loss: 0.1331 - val_accuracy: 0.9550\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9337\n",
      "Epoch 00072: val_loss improved from 0.13309 to 0.13235, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1531 - accuracy: 0.9337 - val_loss: 0.1324 - val_accuracy: 0.9500\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9337\n",
      "Epoch 00073: val_loss improved from 0.13235 to 0.13174, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1521 - accuracy: 0.9337 - val_loss: 0.1317 - val_accuracy: 0.9500\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9350\n",
      "Epoch 00074: val_loss improved from 0.13174 to 0.13120, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1512 - accuracy: 0.9350 - val_loss: 0.1312 - val_accuracy: 0.9500\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9337\n",
      "Epoch 00075: val_loss improved from 0.13120 to 0.13074, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1501 - accuracy: 0.9337 - val_loss: 0.1307 - val_accuracy: 0.9500\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9350\n",
      "Epoch 00076: val_loss improved from 0.13074 to 0.13042, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1490 - accuracy: 0.9350 - val_loss: 0.1304 - val_accuracy: 0.9600\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9400\n",
      "Epoch 00077: val_loss improved from 0.13042 to 0.13017, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1479 - accuracy: 0.9400 - val_loss: 0.1302 - val_accuracy: 0.9600\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9438\n",
      "Epoch 00078: val_loss improved from 0.13017 to 0.12988, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1470 - accuracy: 0.9438 - val_loss: 0.1299 - val_accuracy: 0.9600\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9450\n",
      "Epoch 00079: val_loss improved from 0.12988 to 0.12942, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1461 - accuracy: 0.9450 - val_loss: 0.1294 - val_accuracy: 0.9600\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9450\n",
      "Epoch 00080: val_loss improved from 0.12942 to 0.12876, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1452 - accuracy: 0.9450 - val_loss: 0.1288 - val_accuracy: 0.9600\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9463\n",
      "Epoch 00081: val_loss improved from 0.12876 to 0.12800, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1442 - accuracy: 0.9463 - val_loss: 0.1280 - val_accuracy: 0.9600\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9475\n",
      "Epoch 00082: val_loss improved from 0.12800 to 0.12722, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1432 - accuracy: 0.9475 - val_loss: 0.1272 - val_accuracy: 0.9600\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9475\n",
      "Epoch 00083: val_loss improved from 0.12722 to 0.12651, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1423 - accuracy: 0.9475 - val_loss: 0.1265 - val_accuracy: 0.9600\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9450\n",
      "Epoch 00084: val_loss improved from 0.12651 to 0.12586, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1414 - accuracy: 0.9450 - val_loss: 0.1259 - val_accuracy: 0.9600\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9438\n",
      "Epoch 00085: val_loss improved from 0.12586 to 0.12527, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1405 - accuracy: 0.9438 - val_loss: 0.1253 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9450\n",
      "Epoch 00086: val_loss improved from 0.12527 to 0.12475, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1396 - accuracy: 0.9450 - val_loss: 0.1248 - val_accuracy: 0.9600\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9475\n",
      "Epoch 00087: val_loss improved from 0.12475 to 0.12432, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1387 - accuracy: 0.9475 - val_loss: 0.1243 - val_accuracy: 0.9600\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9500\n",
      "Epoch 00088: val_loss improved from 0.12432 to 0.12393, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.1378 - accuracy: 0.9500 - val_loss: 0.1239 - val_accuracy: 0.9600\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9513\n",
      "Epoch 00089: val_loss improved from 0.12393 to 0.12354, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.1370 - accuracy: 0.9513 - val_loss: 0.1235 - val_accuracy: 0.9600\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9575\n",
      "Epoch 00090: val_loss improved from 0.12354 to 0.12307, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1362 - accuracy: 0.9575 - val_loss: 0.1231 - val_accuracy: 0.9600\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9613\n",
      "Epoch 00091: val_loss improved from 0.12307 to 0.12253, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1354 - accuracy: 0.9613 - val_loss: 0.1225 - val_accuracy: 0.9600\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9613\n",
      "Epoch 00092: val_loss improved from 0.12253 to 0.12192, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1346 - accuracy: 0.9613 - val_loss: 0.1219 - val_accuracy: 0.9600\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9613\n",
      "Epoch 00093: val_loss improved from 0.12192 to 0.12130, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1337 - accuracy: 0.9613 - val_loss: 0.1213 - val_accuracy: 0.9600\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.9625\n",
      "Epoch 00094: val_loss improved from 0.12130 to 0.12071, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1329 - accuracy: 0.9625 - val_loss: 0.1207 - val_accuracy: 0.9600\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9625\n",
      "Epoch 00095: val_loss improved from 0.12071 to 0.12016, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1322 - accuracy: 0.9625 - val_loss: 0.1202 - val_accuracy: 0.9600\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9625\n",
      "Epoch 00096: val_loss improved from 0.12016 to 0.11967, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1314 - accuracy: 0.9625 - val_loss: 0.1197 - val_accuracy: 0.9600\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9625\n",
      "Epoch 00097: val_loss improved from 0.11967 to 0.11922, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1306 - accuracy: 0.9625 - val_loss: 0.1192 - val_accuracy: 0.9600\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9625\n",
      "Epoch 00098: val_loss improved from 0.11922 to 0.11883, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1299 - accuracy: 0.9625 - val_loss: 0.1188 - val_accuracy: 0.9600\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9625\n",
      "Epoch 00099: val_loss improved from 0.11883 to 0.11845, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.1291 - accuracy: 0.9625 - val_loss: 0.1185 - val_accuracy: 0.9600\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9638\n",
      "Epoch 00100: val_loss improved from 0.11845 to 0.11807, saving model to models/lr.h5\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1284 - accuracy: 0.9638 - val_loss: 0.1181 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f42a03b81d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32, 32)),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x,y_one_hot, epochs=100,batch_size=N,verbose=1,\\\n",
    "          validation_data=(test_x,test_y_one_hot),callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [800, 32, 32]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9048879f2172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#           validation_data=(test_x,test_y_one_hot),callbacks=[checkpoint])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /home/jiayu/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [800, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "# build the model and train it\n",
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, (32, 32),activation=\"relu\",input_shape=(32, 32,32)),  \n",
    "#       tf.keras.layers.Dropout(0.1),  # (batch, 42, 1, 8)\n",
    "#       tf.keras.layers.Conv2D(16, (4, 1), padding=\"same\",\n",
    "#                              activation=\"relu\"),  # (batch, 42, 1, 16)\n",
    "#       tf.keras.layers.MaxPool2D((3, 1), padding=\"same\"),  # (batch, 14, 1, 16)\n",
    "#       tf.keras.layers.Dropout(0.1),  # (batch, 14, 1, 16)\n",
    "      tf.keras.layers.Flatten(),  # (batch, 224)\n",
    "#       tf.keras.layers.Dense(16, activation=\"relu\"),  # (batch, 16)\n",
    "#       tf.keras.layers.Dropout(0.1),  # (batch, 16)\n",
    "      # tf.keras.layers.Dense(8, activation=\"relu\"),  # (batch, 4)\n",
    "      tf.keras.layers.Dense(2, activation=\"sigmoid\")  # (batch, 4)\n",
    "  ])\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x,y_one_hot, epochs=100,batch_size=N,verbose=1)\n",
    "#           validation_data=(test_x,test_y_one_hot),callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
